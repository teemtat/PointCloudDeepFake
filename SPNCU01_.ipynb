{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from mtcnn import MTCNN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
    "import numpy.linalg as LA\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jr2xCtN7ezyU",
    "outputId": "0a87e1e8-d4b0-40e8-d9da-3bc2300e0d86"
   },
   "outputs": [],
   "source": [
    "class EnhancedFacePredictor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_selector = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.feature_importance = {}\n",
    "        self.optimal_features = []\n",
    "\n",
    "        # troy modify\n",
    "        self.base_fusion_weights = {\n",
    "            'weighted_score': 0.75,\n",
    "            'random_forest': 0.05,\n",
    "            'xgboost': 0.05,\n",
    "            'svm': 0.05,\n",
    "            'statistical': 0.10,\n",
    "        }\n",
    "        self._norm_warned_features = set()\n",
    "\n",
    "    ## Need review\n",
    "    \n",
    "    # def _create_advanced_prediction_formula(self, f1_features, f2_features, selected_features, weights):\n",
    "    #     weighted_score_diff = self._calculate_weighted_score(f1_features, f2_features, selected_features, weights)\n",
    "    #     rf_score = self._random_forest_prediction(f1_features, f2_features, selected_features)\n",
    "    #     xgb_score = self._xgboost_prediction(f1_features, f2_features, selected_features)\n",
    "    #     svm_score = self._svm_prediction(f1_features, f2_features, selected_features)\n",
    "    #     statistical_score = self._statistical_ensemble(f1_features, f2_features, selected_features)\n",
    "\n",
    "    #     final_scores = {\n",
    "    #         'weighted_score': weighted_score_diff * 0.75,    # OG: 0.25\n",
    "    #         'random_forest': rf_score * 0.05,                # OG: 0.25\n",
    "    #         'xgboost': xgb_score * 0.05,                     # OG: 0.20\n",
    "    #         'svm': svm_score * 0.05,                         # OG: 0.15\n",
    "    #         'statistical': statistical_score * 0.10          # OG: 0.15\n",
    "    #     }\n",
    "    #     final_prediction = sum(final_scores.values())\n",
    "    #     return final_prediction, final_scores\n",
    "\n",
    "    def _compute_dynamic_weights(self, model_scores: dict) -> dict:\n",
    "\n",
    "        ## Need review\n",
    "        \"\"\"\n",
    "        Compute per-picture fusion weights.\n",
    "        model_scores: dict with keys\n",
    "            ['weighted_score', 'random_forest', 'xgboost', 'svm', 'statistical']\n",
    "            and their raw scores (can be positive/negative).\n",
    "        Returns:\n",
    "            dict with same keys, values sum to 1.\n",
    "        \"\"\"\n",
    "    \n",
    "        # 1) get base weights\n",
    "        base = self.base_fusion_weights\n",
    "    \n",
    "        # 2) compute confidence of each model for THIS picture\n",
    "        #    here we use |score|; you can swap to something more fancy later.\n",
    "        confidences = {k: abs(v) for k, v in model_scores.items()}\n",
    "    \n",
    "        # 3) combine base weight and confidence\n",
    "        #    dynamic_weight_raw = base_weight * confidence\n",
    "        raw = {}\n",
    "        for name in model_scores.keys():\n",
    "            bw = base.get(name, 0.0)\n",
    "            conf = confidences.get(name, 0.0)\n",
    "    \n",
    "            # small epsilon to avoid all-zero in pathological cases\n",
    "            raw[name] = bw * (conf + 1e-6)\n",
    "    \n",
    "        # 4) normalize so that sum(weights) = 1\n",
    "        total = sum(raw.values())\n",
    "        if total <= 0:\n",
    "            # fallback: just use base weights\n",
    "            return base.copy()\n",
    "    \n",
    "        dyn = {name: val / total for name, val in raw.items()}\n",
    "        return dyn\n",
    "\n",
    "    def _create_advanced_prediction_formula (self, f1_features, f2_features, selected_features, weights):\n",
    "        weighted_score_diff = self._calculate_weighted_score(f1_features, f2_features, selected_features, weights)\n",
    "        rf_score  = self._random_forest_prediction(f1_features, f2_features, selected_features)\n",
    "        xgb_score = self._xgboost_prediction(f1_features, f2_features, selected_features)\n",
    "        svm_score = self._svm_prediction(f1_features, f2_features, selected_features)\n",
    "        statistical_score = self._statistical_ensemble(f1_features, f2_features, selected_features)\n",
    "    \n",
    "        # raw per-model scores for THIS picture\n",
    "        model_scores = {\n",
    "            'weighted_score': weighted_score_diff,\n",
    "            'random_forest': rf_score,\n",
    "            'xgboost': xgb_score,\n",
    "            'svm': svm_score,\n",
    "            'statistical': statistical_score,\n",
    "        }\n",
    "    \n",
    "        # 2) get dynamic weights for THIS picture\n",
    "        dynamic_weights = self._compute_dynamic_weights(model_scores)\n",
    "    \n",
    "        # 3) fuse scores using dynamic weights\n",
    "        final_prediction = sum(\n",
    "            dynamic_weights[name] * score\n",
    "            for name, score in model_scores.items()\n",
    "        )\n",
    "    \n",
    "        # (optional) return both for analysis/export\n",
    "        return final_prediction, {\n",
    "            **model_scores,\n",
    "            **{f\"w_{k}\": v for k, v in dynamic_weights.items()},\n",
    "        }\n",
    "\n",
    "    def _calculate_weighted_score(self, f1_features, f2_features, selected_features, weights):\n",
    "        score_diff = 0\n",
    "        for feature in selected_features:\n",
    "            if feature in f1_features and feature in f2_features:\n",
    "                normalized_diff = self._dynamic_normalization(f1_features[feature], f2_features[feature], feature)\n",
    "                weight = weights.get(feature, 0.0)\n",
    "                score_diff += weight * normalized_diff\n",
    "        return score_diff\n",
    "\n",
    "    def _dynamic_normalization(self, val1, val2, feature_name):\n",
    "        normalization_params = {\n",
    "            'focus_measure': {'max_val': 1000, 'boost_factor': 2.0},\n",
    "            'texture_variation': {'max_val': 500, 'boost_factor': 1.8},\n",
    "            'edge_density': {'max_val': 0.1, 'boost_factor': 3.0},\n",
    "            'border_cohesion': {'max_val': 0.5, 'boost_factor': 2.5},\n",
    "            'structural_entropy': {'max_val': 1.0, 'boost_factor': 2.2},\n",
    "            'anisotropy': {'max_val': 1.0, 'boost_factor': 1.5},\n",
    "            'surface_roughness': {'max_val': 1.0, 'boost_factor': 2.0},\n",
    "            'multiscale_complexity': {'max_val': 0.01, 'boost_factor': 4.0},\n",
    "            'cluster_coherence': {'max_val': 0.5, 'boost_factor': 2.0},\n",
    "            'linearity': {'max_val': 0.1, 'boost_factor': 3.0},\n",
    "            'planarity': {'max_val': 0.5, 'boost_factor': 1.8},\n",
    "            'scattering': {'max_val': 0.3, 'boost_factor': 2.0},\n",
    "            'depth_variance': {'max_val': 3000, 'boost_factor': 1.5},\n",
    "            'high_frequency_content': {'max_val': 300, 'boost_factor': 1.8},\n",
    "            'blur_consistency': {'max_val': 500, 'boost_factor': 1.2},\n",
    "            'frequency_std': {'max_val': 40, 'boost_factor': 1.3},\n",
    "            'color_consistency': {'max_val': 100, 'boost_factor': 1.1},\n",
    "            'skin_tone_consistency': {'max_val': 1.0, 'boost_factor': 1.2},\n",
    "            'border_color_density_cohesion': {'max_val': 100, 'boost_factor': 1.1},\n",
    "            'border_edge_cohesion': {'max_val': 0.1, 'boost_factor': 2.0}\n",
    "        }\n",
    "        params = normalization_params.get(feature_name, {'max_val': 1.0, 'boost_factor': 1.0})\n",
    "        diff = val1 - val2\n",
    "        normalized_diff = diff / params['max_val']\n",
    "        if abs(diff) > (params['max_val'] * 0.1):\n",
    "            normalized_diff *= params['boost_factor']\n",
    "        return np.clip(normalized_diff, -1, 1)\n",
    "\n",
    "## Need review\n",
    "    # def fit_normalization_stats(self, feature_rows, feature_names=None, min_count=3):\n",
    "    #     \"\"\"Fit per-feature scaling from training rows.\n",
    "\n",
    "    #     feature_rows: list of dicts (one dict per face) with numeric values.\n",
    "    #     feature_names: optional iterable of expected feature names (e.g., weight keys).\n",
    "    #     min_count: minimum samples needed before using data-driven scale; otherwise defaults to 1.0.\n",
    "    #     \"\"\"\n",
    "    #     if not feature_rows and not feature_names:\n",
    "    #         self.normalization_stats = {}\n",
    "    #         return\n",
    "    #     names = set(feature_names or [])\n",
    "    #     for row in feature_rows:\n",
    "    #         names.update(row.keys())\n",
    "    #     self.normalization_stats = {}\n",
    "    #     for feat in sorted(names):\n",
    "    #         vals = [row[feat] for row in feature_rows if feat in row]\n",
    "    #         if len(vals) < max(1, min_count):\n",
    "    #             # fallback scale when not enough samples\n",
    "    #             self.normalization_stats[feat] = {\"scale\": 1.0}\n",
    "    #             continue\n",
    "    #         vals_arr = np.array(vals)\n",
    "    #         iqr = np.subtract(*np.percentile(vals_arr, [75, 25]))\n",
    "    #         std = np.std(vals_arr)\n",
    "    #         # robust scale: prefer IQR, fall back to std, never below a small floor\n",
    "    #         scale = max(iqr / 1.349, std, 1e-3)\n",
    "    #         self.normalization_stats[feat] = {\"scale\": scale}\n",
    "\n",
    "    # def _dynamic_normalization(self, val1, val2, feature_name):\n",
    "    #     stats = getattr(self, \"normalization_stats\", {}).get(feature_name)\n",
    "    #     if not stats:\n",
    "    #         warned = getattr(self, \"_norm_warned_features\", set())\n",
    "    #         if feature_name not in warned:\n",
    "    #             print(f\"[warn] No normalization stats for {feature_name}; using fallback tanh scaling.\")\n",
    "    #             warned.add(feature_name)\n",
    "    #             self._norm_warned_features = warned\n",
    "    #         return float(np.tanh((val1 - val2) / 3.0))\n",
    "    #     diff = val1 - val2\n",
    "    #     scaled = diff / (stats[\"scale\"] + 1e-6)\n",
    "    #     # smooth clip to keep ordering while bounding influence\n",
    "    #     return float(np.tanh(scaled / 3.0))\n",
    "            \n",
    "    def _random_forest_prediction(self, f1_features, f2_features, selected_features):\n",
    "        try:\n",
    "            X = np.array([[f1_features[feat] for feat in selected_features],\n",
    "                            [f2_features[feat] for feat in selected_features]])\n",
    "            y = np.array([1, 0])\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            rf.fit(X, y)\n",
    "            self.feature_importance['random_forest'] = dict(zip(selected_features, rf.feature_importances_))\n",
    "            proba = rf.predict_proba(X)[0, 0]\n",
    "            return (proba - 0.5) * 2\n",
    "        except Exception as e:\n",
    "            print(f\"Random Forest prediction failed: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _xgboost_prediction(self, f1_features, f2_features, selected_features):\n",
    "        try:\n",
    "            X = np.array([[f1_features[feat] for feat in selected_features],\n",
    "                            [f2_features[feat] for feat in selected_features]])\n",
    "            y = np.array([1, 0])\n",
    "            xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
    "            xgb.fit(X, y)\n",
    "            self.feature_importance['xgboost'] = dict(zip(selected_features, xgb.feature_importances_))\n",
    "            proba = xgb.predict_proba(X)[0, 0]\n",
    "            return (proba - 0.5) * 2\n",
    "        except Exception as e:\n",
    "            print(f\"XGBoost prediction failed: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _svm_prediction(self, f1_features, f2_features, selected_features):\n",
    "        try:\n",
    "            X = np.array([[f1_features[feat] for feat in selected_features],\n",
    "                            [f2_features[feat] for feat in selected_features]])\n",
    "            y = np.array([1, 0])\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "            svm.fit(X_scaled, y)\n",
    "            proba = svm.predict_proba(X_scaled)[0, 0]\n",
    "            return (proba - 0.5) * 2\n",
    "        except Exception as e:\n",
    "            print(f\"SVM prediction failed: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _statistical_ensemble(self, f1_features, f2_features, selected_features):\n",
    "        statistical_measures = []\n",
    "        for feature in selected_features:\n",
    "            if feature in f1_features and feature in f2_features:\n",
    "                val1, val2 = f1_features[feature], f2_features[feature]\n",
    "                z_score = (val1 - val2) / (np.std([val1, val2]) + 1e-8)\n",
    "                effect_size = (val1 - val2) / (np.std([val1, val2]) + 1e-8)\n",
    "                percent_diff = (val1 - val2) / (max(abs(val1), abs(val2)) + 1e-8)\n",
    "                combined_measure = np.mean([z_score, effect_size, percent_diff])\n",
    "                statistical_measures.append(combined_measure)\n",
    "        return np.mean(statistical_measures) if statistical_measures else 0\n",
    "\n",
    "    def select_optimal_features(self, f1_features, f2_features, all_features, target_ratio=0.6):\n",
    "        feature_scores = {}\n",
    "        for feature in all_features:\n",
    "            if feature in f1_features and feature in f2_features:\n",
    "                score = 0\n",
    "                val1, val2 = f1_features[feature], f2_features[feature]\n",
    "                discriminative_power = abs(val1 - val2) / (abs(val1) + abs(val2) + 1e-8)\n",
    "                score += discriminative_power * 0.4\n",
    "                magnitude = max(abs(val1), abs(val2))\n",
    "                if magnitude > 1e-5:\n",
    "                    score += min(1.0, magnitude) * 0.3\n",
    "                else:\n",
    "                    score -= 0.5\n",
    "                stability = 1.0 - (abs(val1 - val2) / (max(abs(val1), abs(val2)) + 1e-8))\n",
    "                score += stability * 0.3\n",
    "                feature_scores[feature] = score\n",
    "        n_features = max(5, int(len(all_features) * target_ratio))\n",
    "        optimal_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)[:n_features]\n",
    "        self.optimal_features = [feat for feat, score in optimal_features]\n",
    "        print(\"\\n=== OPTIMAL FEATURE SELECTION ===\")\n",
    "        print(\"Selected Features:\")\n",
    "        for feat, score in optimal_features:\n",
    "            print(f\"  {feat}: {score:.3f} (Face1: {f1_features[feat]:.3f}, Face2: {f2_features[feat]:.3f})\")\n",
    "        return self.optimal_features\n",
    "\n",
    "    def calculate_advanced_confidence(self, prediction_scores, selected_features, f1_features, f2_features):\n",
    "        confidence_factors = {}\n",
    "        predictions = list(prediction_scores.values())\n",
    "        consistency = 1.0 - (np.std(predictions) / (np.mean(np.abs(predictions)) + 1e-8))\n",
    "        confidence_factors['method_consistency'] = consistency * 0.3\n",
    "        agreeing_features = 0\n",
    "        for feature in selected_features:\n",
    "            if (prediction_scores['weighted_score'] > 0 and f1_features[feature] > f2_features[feature]) or \\\n",
    "                (prediction_scores['weighted_score'] < 0 and f2_features[feature] > f1_features[feature]):\n",
    "                agreeing_features += 1\n",
    "        feature_agreement = agreeing_features / len(selected_features)\n",
    "        confidence_factors['feature_agreement'] = feature_agreement * 0.25\n",
    "        avg_prediction_magnitude = np.mean([abs(score) for score in predictions])\n",
    "        confidence_factors['prediction_strength'] = avg_prediction_magnitude * 0.25\n",
    "        feature_quality = len([f for f in selected_features if max(f1_features[f], f2_features[f]) > 0.01])\n",
    "        feature_quality_ratio = feature_quality / len(selected_features)\n",
    "        confidence_factors['feature_quality'] = feature_quality_ratio * 0.2\n",
    "        raw_confidence = sum(confidence_factors.values())\n",
    "        advanced_confidence = 100 / (1 + np.exp(-10 * (raw_confidence - 0.5)))\n",
    "        return min(100, max(0, advanced_confidence)), confidence_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedFaceAnalyzer:\n",
    "    def __init__(self):\n",
    "        ## Need review\n",
    "        self.authentic_patterns = {\n",
    "            'focus_measure': {'min': 50, 'max': 500, 'preferred': 'higher'},\n",
    "            'texture_variation': {'min': 10, 'max': 100, 'preferred': 'higher'},\n",
    "            'edge_density': {'min': 0.005, 'max': 0.05, 'preferred': 'balanced'},\n",
    "            'depth_variance': {'min': 500, 'max': 3000, 'preferred': 'balanced'},\n",
    "            'high_frequency_content': {'min': 100, 'max': 300, 'preferred': 'higher'},\n",
    "            'blur_consistency': {'min': 50, 'max': 500, 'preferred': 'lower'},\n",
    "            'structural_entropy': {'min': 0.5, 'max': 0.9, 'preferred': 'balanced'},\n",
    "            'surface_roughness': {'min': 0.1, 'max': 0.8, 'preferred': 'higher'},\n",
    "            'cluster_coherence': {'min': 0.3, 'max': 0.8, 'preferred': 'balanced'},\n",
    "            'anisotropy': {'min': 0.3, 'max': 0.7, 'preferred': 'balanced'}\n",
    "        }\n",
    "\n",
    "    def identify_problem_features(self, f1_features, f2_features):\n",
    "        print(\"\\n PROBLEMATIC FEATURE ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        problems = []\n",
    "        zero_features = []\n",
    "        for feature in f1_features:\n",
    "            if f1_features[feature] == 0 and f2_features[feature] == 0:\n",
    "                zero_features.append(feature)\n",
    "            elif abs(f1_features[feature]) < 1e-10 and abs(f2_features[feature]) < 1e-10:\n",
    "                zero_features.append(feature)\n",
    "        if zero_features:\n",
    "            print(\"... ZERO-VALUE FEATURES (likely errors):\")\n",
    "            for feat in zero_features[:5]:\n",
    "                print(f\"   {feat}: Both faces have value 0\")\n",
    "            problems.extend(zero_features)\n",
    "        identical_features = []\n",
    "        for feature in f1_features:\n",
    "            if feature in f2_features:\n",
    "                diff_ratio = abs(f1_features[feature] - f2_features[feature]) / (max(abs(f1_features[feature]), abs(f2_features[feature])) + 1e-8)\n",
    "                if diff_ratio < 0.01:\n",
    "                    identical_features.append((feature, diff_ratio))\n",
    "        if identical_features:\n",
    "            print(\"... NEAR-IDENTICAL FEATURES (no discrimination):\")\n",
    "            for feat, ratio in sorted(identical_features, key=lambda x: x[1])[:5]:\n",
    "                print(f\"   {feat}: Only {ratio:.2%} difference\")\n",
    "            problems.extend([feat for feat, _ in identical_features])\n",
    "        out_of_range = []\n",
    "        for feature, pattern in self.authentic_patterns.items():\n",
    "            if feature in f1_features and feature in f2_features:\n",
    "                val1, val2 = f1_features[feature], f2_features[feature]\n",
    "                if val1 < pattern['min'] or val1 > pattern['max'] or val2 < pattern['min'] or val2 > pattern['max']:\n",
    "                    out_of_range.append((feature, val1, val2, pattern))\n",
    "        if out_of_range:\n",
    "            print(\"... OUT-OF-RANGE FEATURES (suspicious values):\")\n",
    "            for feat, v1, v2, pattern in out_of_range[:5]:\n",
    "                print(f\"   {feat}: Face1={v1:.3f}, Face2={v2:.3f} (expected: {pattern['min']}-{pattern['max']})\")\n",
    "            problems.extend([feat for feat, _, _, _ in out_of_range])\n",
    "        return problems\n",
    "\n",
    "    def create_corrected_prediction(self, f1_features, f2_features):\n",
    "        problematic_features = self.identify_problem_features(f1_features, f2_features)\n",
    "        reliable_features = []\n",
    "        for feature in f1_features:\n",
    "            if feature not in problematic_features:\n",
    "                if (feature in f1_features and feature in f2_features and\n",
    "                    f1_features[feature] != f2_features[feature] and\n",
    "                    max(f1_features[feature], f2_features[feature]) > 0.001):\n",
    "                    reliable_features.append(feature)\n",
    "        print(f\"\\n RELIABLE FEATURES SELECTED ({len(reliable_features)} features):\")\n",
    "        for feat in reliable_features:\n",
    "            diff = f1_features[feat] - f2_features[feat]\n",
    "            advantage = \"Face1\" if diff > 0 else \"Face2\"\n",
    "            print(f\"   {feat}: {abs(diff):.4f} difference ({advantage})\")\n",
    "        face1_score, face2_score = self._calculate_reliable_scores(f1_features, f2_features, reliable_features)\n",
    "        if face1_score > face2_score:\n",
    "            verdict = \"Face 1 (Left) is more authentic\"\n",
    "            advantage = face1_score - face2_score\n",
    "            winner = \"Face 1\"\n",
    "        else:\n",
    "            verdict = \"Face 2 (Right) is more authentic\"\n",
    "            advantage = face2_score - face1_score\n",
    "            winner = \"Face 2\"\n",
    "        confidence = self._calculate_reliable_confidence(f1_features, f2_features, reliable_features)\n",
    "        return {\n",
    "            'face1_score': face1_score,\n",
    "            'face2_score': face2_score,\n",
    "            'verdict': verdict,\n",
    "            'advantage': advantage,\n",
    "            'winner': winner,\n",
    "            'confidence': confidence,\n",
    "            'reliable_features': reliable_features,\n",
    "            'problematic_features': problematic_features\n",
    "        }\n",
    "\n",
    "    def _calculate_reliable_scores(self, f1_features, f2_features, reliable_features):\n",
    "        weights = {}\n",
    "        for feature in reliable_features:\n",
    "            diff_ratio = abs(f1_features[feature] - f2_features[feature]) / (max(abs(f1_features[feature]), abs(f2_features[feature])) + 1e-8)\n",
    "            magnitude = max(abs(f1_features[feature]), abs(f2_features[feature]))\n",
    "            magnitude_weight = min(1.0, magnitude) if magnitude > 0 else 0\n",
    "            weights[feature] = diff_ratio * 0.7 + magnitude_weight * 0.3\n",
    "        total_weight = sum(weights.values())\n",
    "        if total_weight > 0:\n",
    "            weights = {k: v/total_weight for k, v in weights.items()}\n",
    "        face1_score = 0\n",
    "        face2_score = 0\n",
    "        for feature in reliable_features:\n",
    "            weight = weights[feature]\n",
    "            val1, val2 = f1_features[feature], f2_features[feature]\n",
    "            norm1, norm2 = self._normalize_feature(feature, val1, val2)\n",
    "            face1_score += weight * norm1\n",
    "            face2_score += weight * norm2\n",
    "        return face1_score, face2_score\n",
    "\n",
    "    def _normalize_feature(self, feature, val1, val2):\n",
    "        if feature in self.authentic_patterns:\n",
    "            pattern = self.authentic_patterns[feature]\n",
    "            range_size = pattern['max'] - pattern['min']\n",
    "            norm1 = (val1 - pattern['min']) / range_size\n",
    "            norm2 = (val2 - pattern['min']) / range_size\n",
    "            norm1 = max(0, min(1, norm1))\n",
    "            norm2 = max(0, min(1, norm2))\n",
    "            if pattern['preferred'] == 'lower':\n",
    "                norm1 = 1 - norm1\n",
    "                norm2 = 1 - norm2\n",
    "            return norm1, norm2\n",
    "        else:\n",
    "            max_val = max(abs(val1), abs(val2), 1e-8)\n",
    "            return val1 / max_val, val2 / max_val\n",
    "\n",
    "    def _calculate_reliable_confidence(self, f1_features, f2_features, reliable_features):\n",
    "        if not reliable_features:\n",
    "            return 0\n",
    "        confidence_factors = []\n",
    "        num_features_confidence = min(1.0, len(reliable_features) / 10)\n",
    "        confidence_factors.append(num_features_confidence * 0.2)\n",
    "        discriminative_powers = []\n",
    "        for feature in reliable_features:\n",
    "            diff_ratio = abs(f1_features[feature] - f2_features[feature]) / (max(abs(f1_features[feature]), abs(f2_features[feature])) + 1e-8)\n",
    "            discriminative_powers.append(diff_ratio)\n",
    "        avg_discriminative = np.mean(discriminative_powers) if discriminative_powers else 0\n",
    "        confidence_factors.append(avg_discriminative * 0.4)\n",
    "        agreeing_features = sum(1 for f in reliable_features if f1_features[f] > f2_features[f])\n",
    "        agreement_ratio = agreeing_features / len(reliable_features)\n",
    "        confidence_factors.append(agreement_ratio * 0.4)\n",
    "        raw_confidence = sum(confidence_factors)\n",
    "        return min(100, max(0, raw_confidence * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedFaceAnalyzerTwoFaces:\n",
    "    def __init__(self, detection_mode='balanced', subsample_rate=3):\n",
    "        self.detection_mode = detection_mode\n",
    "        self.subsample_rate = subsample_rate\n",
    "        self.haar_cascade = None\n",
    "        self.mtcnn_detector = None\n",
    "        self._last_detection_method = None\n",
    "        self.predictor = EnhancedFacePredictor()\n",
    "        self.fixer = FixedFaceAnalyzer()\n",
    "\n",
    "    def _init_detectors(self):\n",
    "        if self.detection_mode != 'accuracy' and self.haar_cascade is None:\n",
    "            self.haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        if self.detection_mode != 'speed' and self.mtcnn_detector is None:\n",
    "            try:\n",
    "                self.mtcnn_detector = MTCNN()\n",
    "            except Exception as e:\n",
    "                print(f\"MTCNN initialization failed: {e}\")\n",
    "                self.mtcnn_detector = None\n",
    "\n",
    "    def _ensure_rgb(self, image):\n",
    "        if len(image.shape) == 3 and image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        elif len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        return image\n",
    "\n",
    "    def detect_faces(self, image_rgb):\n",
    "        self._init_detectors()\n",
    "        image_rgb = self._ensure_rgb(image_rgb)\n",
    "        if self.detection_mode == 'speed':\n",
    "            self._last_detection_method = 'haar'\n",
    "            gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
    "            faces = self.haar_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "            return [{'box': [x, y, w, h]} for (x, y, w, h) in faces]\n",
    "            \n",
    "        elif self.detection_mode == 'accuracy':\n",
    "            if self.mtcnn_detector is None:\n",
    "                return self.detect_faces(image_rgb)\n",
    "            self._last_detection_method = 'mtcnn'\n",
    "            try:\n",
    "                faces = self.mtcnn_detector.detect_faces(image_rgb)\n",
    "                return faces\n",
    "            except Exception as e:\n",
    "                print(f\"MTCNN detection failed: {e}\")\n",
    "                return []\n",
    "                \n",
    "        else:\n",
    "            if self.mtcnn_detector is not None:\n",
    "                try:\n",
    "                    faces = self.mtcnn_detector.detect_faces(image_rgb)\n",
    "                    if len(faces) > 0:\n",
    "                        self._last_detection_method = 'mtcnn'\n",
    "                        return faces\n",
    "                except Exception as e:\n",
    "                    print(f\"MTCNN detection failed: {e}\")\n",
    "            self._last_detection_method = 'haar'\n",
    "            gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)\n",
    "            faces = self.haar_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "            return [{'box': [x, y, w, h]} for (x, y, w, h) in faces]\n",
    "\n",
    "    def _frequency_domain_analysis(self, gray_face):\n",
    "        try:\n",
    "            f_transform = np.fft.fft2(gray_face)\n",
    "            f_shift = np.fft.fftshift(f_transform)\n",
    "            magnitude_spectrum = 20 * np.log(np.abs(f_shift)) + 1\n",
    "            freq_std = np.std(magnitude_spectrum)\n",
    "            freq_mean = np.mean(magnitude_spectrum)\n",
    "            rows, cols = gray_face.shape\n",
    "            crow, ccol = rows // 2, cols // 2\n",
    "            high_freq_region = magnitude_spectrum[crow-10:crow+10, ccol-10:ccol+10]\n",
    "            high_freq_content = np.mean(high_freq_region)\n",
    "            return {\n",
    "                'freq_std': freq_std,\n",
    "                'freq_mean': freq_mean,\n",
    "                'high_freq_content': high_freq_content\n",
    "            }\n",
    "        except:\n",
    "            return {'freq_std': 0, 'freq_mean': 0, 'high_freq_content': 0}\n",
    "\n",
    "    def _color_consistency_analysis(self, face_region):\n",
    "        try:\n",
    "            lab = cv2.cvtColor(face_region, cv2.COLOR_RGB2LAB)\n",
    "            l_channel, a_channel, b_channel = cv2.split(lab)\n",
    "            color_consistency = np.std(a_channel) + np.std(b_channel)\n",
    "            hsv = cv2.cvtColor(face_region, cv2.COLOR_RGB2HSV)\n",
    "            skin_mask = cv2.inRange(hsv, (0, 20, 70), (20, 255, 255))\n",
    "            skin_ratio = np.sum(skin_mask > 0) / (face_region.shape[0] * face_region.shape[1])\n",
    "            return {\n",
    "                'color_consistency': color_consistency,\n",
    "                'skin_tone_consistency': skin_ratio\n",
    "            }\n",
    "        except:\n",
    "            return {'color_consistency': 0, 'skin_tone_consistency': 0}\n",
    "\n",
    "    def _blur_consistency_analysis(self, gray_face):\n",
    "        try:\n",
    "            laplacian_var = cv2.Laplacian(gray_face, cv2.CV_64F).var()\n",
    "            focus_measure = cv2.Laplacian(gray_face, cv2.CV_64F).var()\n",
    "            h, w = gray_face.shape\n",
    "            regions = [\n",
    "                gray_face[0:h//2, 0:w//2],\n",
    "                gray_face[0:h//2, w//2:w],\n",
    "                gray_face[h//2:h, 0:w//2],\n",
    "                gray_face[h//2:h, w//2:w]\n",
    "            ]\n",
    "            region_blurs = [cv2.Laplacian(region, cv2.CV_64F).var() for region in regions]\n",
    "            blur_consistency = np.std(region_blurs)\n",
    "            return {\n",
    "                'laplacian_variance': laplacian_var,\n",
    "                'focus_measure': focus_measure,\n",
    "                'blur_consistency': blur_consistency\n",
    "            }\n",
    "        except:\n",
    "            return {'laplacian_variance': 0, 'focus_measure': 0, 'blur_consistency': 0}\n",
    "\n",
    "    def _process_face_with_cloud(self, image_rgb, face):\n",
    "        ## Need review\n",
    "        x, y, w, h = face['box']\n",
    "        margin = min(w, h) // 3 # troy modify\n",
    "        x, y = max(0, x - margin), max(0, y - margin)\n",
    "        w, h = min(w + 2*margin, image_rgb.shape[1] - x), min(h + 2*margin, image_rgb.shape[0] - y)\n",
    "        face_region = image_rgb[y:y+h, x:x+w]\n",
    "        \n",
    "        gray = cv2.cvtColor(face_region, cv2.COLOR_RGB2GRAY)\n",
    "        laplacian = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        dynamic_rate = max(1, int(self.subsample_rate * (1 + (100 - laplacian) / 100)))\n",
    "\n",
    "        xx, yy = np.meshgrid(np.arange(gray.shape[1]), np.arange(gray.shape[0]))\n",
    "        xx_sub = xx[::dynamic_rate, ::dynamic_rate].flatten()\n",
    "        yy_sub = yy[::dynamic_rate, ::dynamic_rate].flatten()\n",
    "        zz_sub = gray[::dynamic_rate, ::dynamic_rate].flatten()\n",
    "        colors = face_region[::dynamic_rate, ::dynamic_rate].reshape(-1, 3)\n",
    "\n",
    "        # Store point cloud\n",
    "        point_cloud = {\n",
    "            'x': xx_sub.astype(float),\n",
    "            'y': yy_sub.astype(float),\n",
    "            'z': zz_sub.astype(float),\n",
    "            'colors': colors.astype(float) / 255.0\n",
    "        }\n",
    "\n",
    "        depth_var = np.var(zz_sub)\n",
    "        texture_var = cv2.Laplacian(zz_sub.reshape(-1, 1), cv2.CV_64F).var() if zz_sub.size > 0 else 0\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        edge_density = np.sum(edges > 0) / (gray.shape[0] * gray.shape[1]) if gray.size > 0 else 0\n",
    "\n",
    "        hairline_region_height = h // 4\n",
    "        hairline_area = gray[0:hairline_region_height, :]\n",
    "        _, dark_mask = cv2.threshold(hairline_area, 70, 255, cv2.THRESH_BINARY_INV)\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dark_mask, connectivity=8)\n",
    "        border_cohesion = 0\n",
    "        if num_labels > 1:\n",
    "            largest_component_area = sorted(stats[1:, cv2.CC_STAT_AREA], reverse=True)[0]\n",
    "            total_dark_area = np.sum(dark_mask > 0)\n",
    "            if total_dark_area > 0:\n",
    "                cohesion_ratio = largest_component_area / total_dark_area\n",
    "                border_cohesion = cohesion_ratio / (num_labels - 1)\n",
    "\n",
    "        hairline_area_color = face_region[0:hairline_region_height, :]\n",
    "        hairline_area_hsv = cv2.cvtColor(hairline_area_color, cv2.COLOR_RGB2HSV)\n",
    "        h_channel, s_channel, v_channel = cv2.split(hairline_area_hsv)\n",
    "        border_color_density_cohesion = np.std(h_channel) + np.std(s_channel) + np.std(v_channel)\n",
    "\n",
    "        hairline_edges = edges[0:hairline_region_height, :]\n",
    "        border_edge_cohesion = np.sum(hairline_edges > 0) / (hairline_area.shape[0] * hairline_area.shape[1]) if hairline_area.size > 0 else 0\n",
    "\n",
    "        freq_features = self._frequency_domain_analysis(gray)\n",
    "        color_features = self._color_consistency_analysis(face_region)\n",
    "        blur_features = self._blur_consistency_analysis(gray)\n",
    "\n",
    "        features = {\n",
    "            'depth_variance': depth_var,\n",
    "            'texture_variation': texture_var,\n",
    "            'edge_density': edge_density,\n",
    "            'border_cohesion': border_cohesion,\n",
    "            'border_color_density_cohesion': border_color_density_cohesion,\n",
    "            'border_edge_cohesion': border_edge_cohesion,\n",
    "            'point_count': len(zz_sub),\n",
    "            'frequency_std': freq_features['freq_std'],\n",
    "            'high_frequency_content': freq_features['high_freq_content'],\n",
    "            'color_consistency': color_features['color_consistency'],\n",
    "            'skin_tone_consistency': color_features['skin_tone_consistency'],\n",
    "            'blur_consistency': blur_features['blur_consistency'],\n",
    "            'focus_measure': blur_features['focus_measure'],\n",
    "            'laplacian_variance': blur_features['laplacian_variance']\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "            hist = hist.flatten() / (gray.shape[0] * gray.shape[1])\n",
    "            hist = hist[hist > 0]\n",
    "            structural_entropy = -np.sum(hist * np.log2(hist))\n",
    "            features['structural_entropy'] = structural_entropy\n",
    "        except:\n",
    "            features['structural_entropy'] = 0\n",
    "\n",
    "        try:\n",
    "            sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "            surface_roughness = np.mean(gradient_magnitude) / 255.0\n",
    "            features['surface_roughness'] = surface_roughness\n",
    "        except:\n",
    "            features['surface_roughness'] = 0\n",
    "\n",
    "        try:\n",
    "            resized_small = cv2.resize(gray, (gray.shape[1]//2, gray.shape[0]//2))\n",
    "            laplacian_small = cv2.Laplacian(resized_small, cv2.CV_64F).var()\n",
    "            multiscale_complexity = abs(laplacian - laplacian_small) / (laplacian + 1e-8)\n",
    "            features['multiscale_complexity'] = multiscale_complexity\n",
    "        except:\n",
    "            features['multiscale_complexity'] = 0\n",
    "\n",
    "        try:\n",
    "            pixel_values = gray.flatten().reshape(-1, 1)\n",
    "            from sklearn.cluster import KMeans\n",
    "            kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(pixel_values[:1000])\n",
    "            cluster_coherence = 1.0 - (np.std(kmeans.cluster_centers_.flatten()) / 255.0)\n",
    "            features['cluster_coherence'] = cluster_coherence\n",
    "        except:\n",
    "            features['cluster_coherence'] = 0\n",
    "\n",
    "        try:\n",
    "            if len(zz_sub) > 10:\n",
    "                covariance_matrix = np.cov(np.vstack([xx_sub, yy_sub, zz_sub]))\n",
    "                eigenvalues = np.linalg.eigvals(covariance_matrix)\n",
    "                eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "                if eigenvalues[0] > 0:\n",
    "                    linearity = (eigenvalues[0] - eigenvalues[1]) / eigenvalues[0]\n",
    "                    planarity = (eigenvalues[1] - eigenvalues[2]) / eigenvalues[0]\n",
    "                    scattering = eigenvalues[2] / eigenvalues[0]\n",
    "                    features['linearity'] = linearity\n",
    "                    features['planarity'] = planarity\n",
    "                    features['scattering'] = scattering\n",
    "                else:\n",
    "                    features.update({'linearity': 0, 'planarity': 0, 'scattering': 0})\n",
    "            else:\n",
    "                features.update({'linearity': 0, 'planarity': 0, 'scattering': 0})\n",
    "        except:\n",
    "            features.update({'linearity': 0, 'planarity': 0, 'scattering': 0})\n",
    "\n",
    "        try:\n",
    "            gabor_responses = []\n",
    "            for theta in [0, np.pi/4, np.pi/2, 3*np.pi/4]:\n",
    "                kernel = cv2.getGaborKernel((21, 21), 8.0, theta, 10.0, 0.5, 0, ktype=cv2.CV_32F)\n",
    "                filtered = cv2.filter2D(gray, cv2.CV_8UC3, kernel)\n",
    "                gabor_responses.append(np.mean(filtered))\n",
    "            anisotropy = np.std(gabor_responses) / (np.mean(gabor_responses) + 1e-8)\n",
    "            features['anisotropy'] = anisotropy\n",
    "        except:\n",
    "            features['anisotropy'] = 0\n",
    "\n",
    "        return features, point_cloud\n",
    "\n",
    "    def analyze_single_image_two_faces(self, image_path):\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not load image: {image_path}\")\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            faces = self.detect_faces(img_rgb)\n",
    "            if len(faces) < 2:\n",
    "                print(f\"!!! Only {len(faces)} face(s) detected. Using top 2 bounding boxes if possible.\")\n",
    "                faces = sorted(faces, key=lambda f: f['box'][0])\n",
    "                if len(faces) == 1:\n",
    "                    faces = [faces[0], faces[0]]\n",
    "                elif len(faces) == 0:\n",
    "                    raise ValueError(\"No faces detected!\")\n",
    "\n",
    "            faces = sorted(faces[:2], key=lambda f: f['box'][0])\n",
    "            face1, face2 = faces[0], faces[1]\n",
    "\n",
    "            f1_features, pc1 = self._process_face_with_cloud(img_rgb, face1)\n",
    "            f2_features, pc2 = self._process_face_with_cloud(img_rgb, face2)\n",
    "\n",
    "            self.face_point_clouds = {'face1': pc1, 'face2': pc2}\n",
    "\n",
    "            all_features = list(set(f1_features.keys()) & set(f2_features.keys()))\n",
    "            selected_features = self.predictor.select_optimal_features(f1_features, f2_features, all_features)\n",
    "\n",
    "            weights = {}\n",
    "            for feature in selected_features:\n",
    "                diff = abs(f1_features[feature] - f2_features[feature])\n",
    "                max_val = max(abs(f1_features[feature]), abs(f2_features[feature]), 1e-8)\n",
    "                weights[feature] = diff / max_val\n",
    "\n",
    "            final_prediction, prediction_scores = self.predictor._create_advanced_prediction_formula(\n",
    "                f1_features, f2_features, selected_features, weights\n",
    "            )\n",
    "\n",
    "            advanced_confidence, confidence_factors = self.predictor.calculate_advanced_confidence(\n",
    "                prediction_scores, selected_features, f1_features, f2_features\n",
    "            )\n",
    "\n",
    "            corrected_result = self.fixer.create_corrected_prediction(f1_features, f2_features)\n",
    "\n",
    "            if final_prediction > 0:\n",
    "                verdict = \"Face 1 (Left) is more authentic\"\n",
    "                winner = \"Face 1\"\n",
    "                advantage = final_prediction\n",
    "            else:\n",
    "                verdict = \"Face 2 (Right) is more authentic\"\n",
    "                winner = \"Face 2\"\n",
    "                advantage = -final_prediction\n",
    "\n",
    "            result = {\n",
    "                'verdict': verdict,\n",
    "                'winner': winner,\n",
    "                'advantage_score': advantage,\n",
    "                'confidence': advanced_confidence,\n",
    "                'confidence_factors': confidence_factors,\n",
    "                'prediction_scores': prediction_scores,\n",
    "                'selected_features': selected_features,\n",
    "                'face1_features': f1_features,\n",
    "                'face2_features': f2_features,\n",
    "                'corrected_result': corrected_result,\n",
    "                'point_clouds': self.face_point_clouds\n",
    "            }\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Analysis failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def plot_3d_point_clouds(self, result):\n",
    "        if not result or 'point_clouds' not in result:\n",
    "            print(\"No point cloud data to plot.\")\n",
    "            return\n",
    "\n",
    "        pc1 = result['point_clouds']['face1']\n",
    "        pc2 = result['point_clouds']['face2']\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]],\n",
    "            subplot_titles=(\"Face 1 (Left)\", \"Face 2 (Right)\")\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=pc1['x'], y=pc1['y'], z=pc1['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, color=pc1['colors'], opacity=0.8)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter3d(\n",
    "                x=pc2['x'], y=pc2['y'], z=pc2['z'],\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, color=pc2['colors'], opacity=0.8)\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=\"3D Point Clouds of Detected Faces\",\n",
    "            scene=dict(aspectmode='data'),\n",
    "            scene2=dict(aspectmode='data'),\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    def print_detailed_results(self, result):\n",
    "        if result is None:\n",
    "            print(\"Analysis failed - no results to display\")\n",
    "            return\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ADVANCED FACE AUTHENTICITY ANALYSIS RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n FINAL VERDICT: {result['verdict']}\")\n",
    "        print(f\" Winner: {result['winner']}\")\n",
    "        print(f\" Advantage Score: {result['advantage_score']:.4f}\")\n",
    "        print(f\" Confidence: {result['confidence']:.1f}%\")\n",
    "        print(f\"\\nðŸ” CONFIDENCE BREAKDOWN:\")\n",
    "        for factor, value in result['confidence_factors'].items():\n",
    "            print(f\"   {factor.replace('_', ' ').title()}: {value:.3f}\")\n",
    "        print(f\"\\n PREDICTION METHOD SCORES:\")\n",
    "        for method, score in result['prediction_scores'].items():\n",
    "            print(f\"   {method.replace('_', ' ').title()}: {score:.4f}\")\n",
    "        print(f\"\\n CORRECTED ANALYSIS:\")\n",
    "        corrected = result['corrected_result']\n",
    "        print(f\"   Corrected Verdict: {corrected['verdict']}\")\n",
    "        print(f\"   Corrected Confidence: {corrected['confidence']:.1f}%\")\n",
    "        print(f\"   Reliable Features Used: {len(corrected['reliable_features'])}\")\n",
    "        print(f\"\\nðŸ“ˆ FEATURE COMPARISON (Top 10):\")\n",
    "        selected_features = result['selected_features'][:10]\n",
    "        for feature in selected_features:\n",
    "            f1_val = result['face1_features'][feature]\n",
    "            f2_val = result['face2_features'][feature]\n",
    "            diff = f1_val - f2_val\n",
    "            advantage = \"Face1\" if diff > 0 else \"Face2\"\n",
    "            print(f\"   {feature}: F1={f1_val:.4f}, F2={f2_val:.4f}, Diff={diff:.4f} ({advantage})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis_on_single_image(image_path, detection_mode='speed'):\n",
    "    analyzer = AdvancedFaceAnalyzerTwoFaces(detection_mode=detection_mode)\n",
    "    result = analyzer.analyze_single_image_two_faces(image_path)\n",
    "    if result:\n",
    "        analyzer.print_detailed_results(result)\n",
    "        analyzer.plot_3d_point_clouds(result)\n",
    "    else:\n",
    "        print(\"... Analysis failed.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def export_result_to_csv(result: dict, image_path: str, out_dir: str = \"exports\"):\n",
    "   \n",
    "    # à¸ªà¸£à¹‰à¸²à¸‡ CSV 3 à¹„à¸Ÿà¸¥à¹Œ/à¸£à¸¹à¸›:\n",
    "    #   1) <basename>_feature_comparison.csv  : à¹€à¸—à¸µà¸¢à¸šà¸„à¹ˆà¸²à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œ F1/F2 + diff\n",
    "    #   2) <basename>_reliable_features.csv   : à¸£à¸²à¸¢à¸à¸²à¸£ reliable features à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸ˆà¸£à¸´à¸‡\n",
    "    #   3) <basename>_summary.csv             : à¸ªà¸£à¸¸à¸› verdict / winner / scores / confidence\n",
    "    \n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    base = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    # --- 1) Feature comparison ---\n",
    "    # à¸žà¸¢à¸²à¸¢à¸²à¸¡à¸”à¸¶à¸‡à¸¥à¸´à¸ªà¸•à¹Œ features à¸•à¸²à¸¡à¸¥à¸³à¸”à¸±à¸šà¸—à¸µà¹ˆà¹€à¸¥à¸·à¸­à¸à¹„à¸§à¹‰; à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹€à¸ˆà¸­ à¹ƒà¸Šà¹‰ union à¸‚à¸­à¸‡ keys\n",
    "    selected = result.get(\"selected_features\")\n",
    "    f1 = result.get(\"face1_features\", {}) or {}\n",
    "    f2 = result.get(\"face2_features\", {}) or {}\n",
    "\n",
    "    feature_list = list(selected) if selected else sorted(set(f1.keys()) | set(f2.keys()))\n",
    "    rows = []\n",
    "    for k in feature_list:\n",
    "        v1 = f1.get(k, None)\n",
    "        v2 = f2.get(k, None)\n",
    "        diff = (v1 - v2) if (v1 is not None and v2 is not None) else None\n",
    "        rows.append({\"feature\": k, \"face1\": v1, \"face2\": v2, \"diff\": diff})\n",
    "\n",
    "    df_feat = pd.DataFrame(rows)\n",
    "    df_feat.to_csv(os.path.join(out_dir, f\"{base}_feature_comparison.csv\"), index=False)\n",
    "\n",
    "    # --- 2) Reliable features (à¸–à¹‰à¸²à¸¡à¸µà¸à¸²à¸£à¹à¸à¹‰à¹„à¸‚/à¸à¸£à¸­à¸‡) ---\n",
    "    reliable = []\n",
    "    corrected = result.get(\"corrected_result\") or {}\n",
    "    reliable_features = corrected.get(\"reliable_features\") or result.get(\"reliable_features\")\n",
    "    if reliable_features:\n",
    "        # à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸²à¸£à¸²à¸‡à¸£à¸§à¸¡à¸„à¹ˆà¸² face1/face2/diff à¹€à¸‰à¸žà¸²à¸° reliable\n",
    "        for k in reliable_features:\n",
    "            v1 = f1.get(k, None)\n",
    "            v2 = f2.get(k, None)\n",
    "            diff = (v1 - v2) if (v1 is not None and v2 is not None) else None\n",
    "            reliable.append({\"feature\": k, \"face1\": v1, \"face2\": v2, \"diff\": diff})\n",
    "        pd.DataFrame(reliable).to_csv(os.path.join(out_dir, f\"{base}_reliable_features.csv\"), index=False)\n",
    "\n",
    "    # --- 3) Summary (à¸«à¸™à¸¶à¹ˆà¸‡à¹à¸–à¸§) ---\n",
    "    pred = result.get(\"prediction_scores\", {})\n",
    "    conf = result.get(\"confidence_factors\", {})\n",
    "    summary = {\n",
    "        \"image\": base,\n",
    "        \"verdict\": result.get(\"verdict\"),\n",
    "        \"winner\": result.get(\"winner\"),\n",
    "        \"advantage_score\": result.get(\"advantage_score\"),\n",
    "        \"confidence\": result.get(\"confidence\"),\n",
    "        \"corrected_verdict\": corrected.get(\"verdict\"),\n",
    "        \"corrected_winner\": corrected.get(\"winner\"),\n",
    "        \"corrected_advantage\": corrected.get(\"advantage\"),\n",
    "        \"corrected_confidence\": corrected.get(\"confidence\"),\n",
    "        # method scores\n",
    "        \"weighted_score\": pred.get(\"weighted_score\"),\n",
    "        \"random_forest\": pred.get(\"random_forest\"),\n",
    "        \"xgboost\": pred.get(\"xgboost\"),\n",
    "        \"svm\": pred.get(\"svm\"),\n",
    "        \"statistical\": pred.get(\"statistical\"),\n",
    "        # confidence factors\n",
    "        \"method_consistency\": conf.get(\"method_consistency\"),\n",
    "        \"feature_agreement\": conf.get(\"feature_agreement\"),\n",
    "        \"prediction_strength\": conf.get(\"prediction_strength\"),\n",
    "        \"feature_quality\": conf.get(\"feature_quality\"),\n",
    "    }\n",
    "    pd.DataFrame([summary]).to_csv(os.path.join(out_dir, f\"{base}_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single image processing\n",
    "image_path = \"/Users/teemtat/Documents/MUICT/RESEARCH/old/Dataset/DFPhoto/df01.jpg\"\n",
    "result = run_analysis_on_single_image(image_path, detection_mode='speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Run all image\n",
    "\n",
    "# folder_path = \"/Users/teemtat/Documents/MUICT/RESEARCH/Dataset/DFPhoto\"\n",
    "# export_folder = \"exports/10th (LargeDT margin3 Dynamic Weight adjust)\"   # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ export CSVs\n",
    "\n",
    "# for filename in sorted(os.listdir(folder_path)):\n",
    "#     if filename.lower().endswith(('.jpg', '.png')):\n",
    "#         image_path = os.path.join(folder_path, filename)\n",
    "#         print(f\"ðŸš€ Analyzing {filename} ...\")\n",
    "#         try:\n",
    "#             result = run_analysis_on_single_image(image_path, detection_mode='speed')\n",
    "#             export_result_to_csv(result, image_path, out_dir=export_folder)\n",
    "#             print(f\"âœ… Exported results for {filename}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"âŒ Error processing {filename}: {e}\")\n",
    "\n",
    "# print(\"ðŸŽ‰ Done! All images analyzed and exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyzer = AdvancedFaceAnalyzerTwoFaces(detection_mode=\"speed\")\n",
    "result = analyzer.analyze_single_image_two_faces(image_path)\n",
    "\n",
    "analyzer.plot_3d_point_clouds(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pcs = getattr(analyzer, \"face_point_clouds\", None) or result.get(\"face_point_clouds\", None)\n",
    "if pcs is None:\n",
    "    raise RuntimeError(\"à¹„à¸¡à¹ˆà¸žà¸š point cloud â€“ à¹ƒà¸«à¹‰à¸£à¸±à¸™ analyze_single_image_two_faces(...) à¸à¹ˆà¸­à¸™\")\n",
    "\n",
    "if isinstance(pcs, dict):\n",
    "    try:\n",
    "        pcs = [pcs[k] for k in sorted(pcs.keys())]\n",
    "    except Exception:\n",
    "        pcs = list(pcs.values())\n",
    "elif not isinstance(pcs, (list, tuple)):\n",
    "    pcs = [pcs]\n",
    "\n",
    "use_ellipse_mask = True\n",
    "step = 2\n",
    "\n",
    "fig = make_subplots(rows=1, cols=len(pcs), specs=[[{'type':'scene'}]*len(pcs)],\n",
    "                    subplot_titles=[f\"Face {i+1}\" for i in range(len(pcs))])\n",
    "\n",
    "for i, pc in enumerate(pcs, start=1):\n",
    "    x = np.asarray(pc['x'], dtype=float)\n",
    "    y = np.asarray(pc['y'], dtype=float)\n",
    "    z = np.asarray(pc['z'], dtype=float)\n",
    "    col = (np.asarray(pc['colors'])*255).astype(np.uint8)\n",
    "\n",
    "    if use_ellipse_mask:\n",
    "        W = x.max() + 1.0; H = y.max() + 1.0\n",
    "        cx, cy = W/2.0, H/2.0\n",
    "        ax, ay = 0.45*W, 0.60*H\n",
    "        m = (((x - cx)/ax)**2 + ((y - cy)/ay)**2) <= 1.0\n",
    "    else:\n",
    "        m = np.ones_like(x, bool)\n",
    "\n",
    "    idx = np.where(m)[0][::step]\n",
    "    colors = [f\"rgb({r},{g},{b})\" for r, g, b in col[idx]]\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(x=x[idx], y=y[idx], z=z[idx],\n",
    "                               mode='markers',\n",
    "                               marker=dict(size=1, color=colors),\n",
    "                               name=f\"Face {i}\"),\n",
    "                  row=1, col=i)\n",
    "\n",
    "    fig.update_scenes(dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'), row=1, col=i)\n",
    "\n",
    "fig.update_layout(height=500, width=980, showlegend=False, margin=dict(l=0,r=0,t=30,b=0))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# export_result_to_csv(result, image_path, out_dir=\"exports\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "SPNCU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
